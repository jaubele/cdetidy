
# This is a list of useful functions required for cleaning and prepping data for REDI data warehouse
# Last updated: 08.12.25 by JA

library(psych)
library(tidyverse)
library(janitor)
library(data.table)
library(openxlsx)
library(stringr)
library(readr)
library(glue)

# CDS 0 padding function, run this to load function

pad_cds_codes <- function(df,
                          county_col = "county_code",
                          district_col = "district_code",
                          school_col = "school_code",
                          create_cds = TRUE,
                          cds_col = "cds") {
  df <- df %>%
    mutate(
      !!county_col   := str_pad(as.character(.data[[county_col]]),   width = 2, side = "left", pad = "0"),
      !!district_col := str_pad(as.character(.data[[district_col]]), width = 5, side = "left", pad = "0"),
      !!school_col   := str_pad(as.character(.data[[school_col]]),   width = 7, side = "left", pad = "0")
    )

  if (create_cds) {
    df <- df %>%
      mutate(
        !!cds_col := paste0(.data[[county_col]], .data[[district_col]], .data[[school_col]])
      )
  }

  return(df)
}

# For Dashboard files, use this function to separate CDS code into county, district, school codes

expand_cds_codes <- function(df,
                             cds_col = "cds",
                             county_col = "county_code",
                             district_col = "district_code",
                             school_col = "school_code") {
  df <- df %>%
    mutate(
      !!county_col   := str_pad(str_sub(.data[[cds_col]], 1, 2), 2, pad = "0"),
      !!district_col := str_pad(str_sub(.data[[cds_col]], 3, 7), 5, pad = "0"),
      !!school_col   := str_pad(str_sub(.data[[cds_col]], 8, 14), 7, pad = "0")
    )

  return(df)
}

# Not new function, but helpful renaming argument for batches of columns with a clear pattern
###########################

# rename_with()
# str_replace or str_replace_all will overwrite strings
# the first argument is the col (the .x) where your string exists
# the second argument is the string that exists = new_string
# lhs (left hand side) is existing name, rhs (right hand side) is new name

#rename_with(~ stringr::str_replace_all(.x, c(
#  "^life_sciences_"                    = "a1_", # convert science names into a1_, a2_, etc
#  "^physical_sciences_"                = "a2_",
#  "^earth_and_space_sciences_"         = "a3_",
#  "domain_percent_below_standard$"     = "perc_below", # convert domain, percent, and level into perc_below, perc_near, etc
#  "domain_percent_near_standard$"      = "perc_near",
#  "domain_percent_above_standard$"     = "perc_above",
#  "domain_count_below_standard$"       = "count_below",
#  "domain_count_near_standard$"        = "count_near",
#  "domain_count_above_standard$"       = "count_above",
#  "domain_total$"                      = "total"))) # change total for domain to a1_total, etc


# Functions for exporting and saving cleaned data warehouse tables
###############################
# safe_fwrite will write the flat csv data file

# Run these functions for exporting files
#######################

validate_sql_identifiers <- function(name_vec, type = c("column", "table"), max_length = 128) {
  type <- match.arg(type)

  # Pattern: starts with letter or underscore, and contains only letters, numbers, or underscores
  invalid_pattern <- !grepl("^[A-Za-z_][A-Za-z0-9_]*$", name_vec)
  too_long <- nchar(name_vec) > max_length

  if (any(invalid_pattern)) {
    stop("\033[31m❌ Invalid SQL ", type, " name(s): ",
         paste(name_vec[invalid_pattern], collapse = ", "),
         "\n⚠️ Must start with a letter or underscore, and contain only letters, numbers, or underscores.\033[0m")
  }

  if (any(too_long)) {
    stop("\033[31m❌ ", type, " name(s) exceed ", max_length, " characters: ",
         paste(name_vec[too_long], collapse = ", "),
         "\n⚠️ Consider shortening these names for SQL compatibility.\033[0m")
  }
}

safe_fwrite <- function(
    data, path = NULL,
    char_cols = c("cds","county_code","district_code","school_code"),
    compress = FALSE,
    n_check = 6,
    log_metadata = NULL,
    data_year = NULL,
    data_source = NULL,
    data_type = NULL,         # validated below
    data_description = NA,
    user_note = NA,
    table_name = NULL,
    dim_description = NULL,
    log_path = "export_log.csv",
    canonical_table_id = NULL,
    dimension_type = NULL) {

  `%||%` <- function(x, y) if (is.null(x) || (length(x) == 1 && is.na(x))) y else x

  norm_token <- function(x) gsub("[^a-z0-9]+", "", tolower(trimws(as.character(x))))

  title_underscore <- function(x) gsub("\\s+", "_", tools::toTitleCase(gsub("_", " ", x)))

  # --- catalogs & resolver ---
  catalog <- list(
    Assessment = list(
      values = c("SBAC","CAST","ELPAC", "dim"),
      syns = list(
        SBAC  = c("sbac","smarter","smarterbalanced"),
        CAST  = c("cast","science"),
        ELPAC = c("elpac","englishlanguageproficiency","elpa"),
        dim = c("dim", "dimension")
      )
    ),
    CDE = list(
      values = c("Absenteeism","Enrollment","Discipline","EL","Grad_Dropout","Post_Secondary", "dim"),
      syns = list(
        Absenteeism    = c("absenteeism","chronic","chronicabsenteeism"),
        Enrollment     = c("enrollment","enrol"),
        Discipline     = c("discipline","suspension","suspensions"),
        EL             = c("el","englishlearner","ell","englishlearners"),
        Grad_Dropout   = c("graddropout","graduation","grad","dropout","cohort"),
        Post_Secondary = c("postsecondary","post_secondary","collegecareer","cci","collegeandcareer"),
        dim = c("dim", "dimension")
      )
    ),
    Dashboard = list(
      values = c("Achievement", "Engagement", "Climate", "Broad_Course", "Info_Only", "dim"),
      syns = list(
        Achievement = c("ela", "math", "elpi", "academics"),
        Engagement = c("grad", "grad_rate", "gr", "absenteeism", "ca", "chronic"),
        Climate = c("suspension","sus", "susp"),
        Broad_Course = c("cci", "college_and_career", "college", "career"),
        Info_Only = c("science", "sci", "growth_rate", "growth"),
        dim = c("dim", "dimension"))
    )
  )

  resolve_type <- function(ds_label, dt_input) {
    catg <- catalog[[ds_label]]
    if (is.null(catg)) stop("❌ Unsupported data_source catalog: ", ds_label)
    tok <- norm_token(dt_input)
    direct <- match(tok, norm_token(catg$values))
    if (!is.na(direct)) return(catg$values[direct])
    for (v in names(catg$syns)) if (tok %in% norm_token(catg$syns[[v]])) return(v)
    stop("❌ For data_source=", ds_label,
         " the `data_type` must be one of: ",
         paste(catg$values, collapse=", "),
         " (case-insensitive; synonyms accepted).")
  }

  # --- build/validate metadata ---
  if (is.null(log_metadata)) {
    if (is.null(data_source)) stop("❌ Provide `data_source` (or a `log_metadata` list).")
    log_metadata <- list(
      data_year       = data_year,        # optional, for audit only
      data_source     = data_source,
      data_description= data_description,
      user_note       = user_note
    )
  }
  if (is.null(log_metadata$data_description) || is.na(log_metadata$data_description) || log_metadata$data_description == "")
    stop("❌ Please provide `data_description`.")

  ds_label <- (function(x){
    tok <- norm_token(x)
    out <- c(assessment="Assessment", cde="CDE", dashboard="Dashboard")[tok]
    if (is.na(out)) stop("❌ `data_source` must be one of: Assessment, CDE, Dashboard.")
    out
  })(log_metadata$data_source)

  if (is.null(table_name)) stop("❌ Please supply `table_name`.")
  if (is.null(user_note) || !grepl("\\b(fact|dim)\\b", user_note, ignore.case = TRUE))
    stop("❌ `user_note` must include 'fact' or 'dim'.")
  table_type <- tolower(stringr::str_extract(user_note, "\\b(fact|dim)\\b"))
  if (is.na(table_type)) stop("❌ Could not parse table type from `user_note`.")
  if (!is.null(dim_description)) dim_description <- janitor::make_clean_names(dim_description)

  final_table_name <- paste0(
    table_name,
    if (!is.null(dim_description)) paste0("_", dim_description),
    "_", table_type
  )

  # --- auto path: T:/Data Warehouse/{DataSource}/{DataType}/ ---
  if (is.null(path)) {
    if (is.null(data_type)) stop("❌ Please provide `data_type` when `path` is NULL.")
    type_label  <- resolve_type(ds_label, data_type)
    type_folder <- title_underscore(type_label)
    base_dir    <- "T:/Data Warehouse"
    path <- file.path(base_dir, ds_label, type_folder, paste0(final_table_name, ".csv"))
    dir.create(dirname(path), recursive = TRUE, showWarnings = FALSE)
  }
  if (compress && !grepl("\\.gz$", path)) path <- paste0(path, ".gz")

  # --- enforce character on code columns ---
  default_char_cols <- if (table_type == "dim")
    c("cds","county_code","district_code","school_code") else c("cds")
  target_char_cols <- intersect(unique(c(char_cols, default_char_cols)), names(data))
  if (length(target_char_cols))
    data <- dplyr::mutate(data, dplyr::across(dplyr::all_of(target_char_cols), as.character))

  # --- write + quick preview ---
  data.table::fwrite(data, path)
  message("✅ File written: ", path)
  if (n_check > 0)
    print(data.table::fread(path, nrows = n_check, colClasses = list(character = target_char_cols)))

  # --- logging ---
  fi <- file.info(path)
  if (is.na(fi$size)) stop("File does not exist or is unreadable: ", path)

  canonical_table_id <- canonical_table_id %||% table_name
  dimension_type <- dimension_type %||% NA
  valid_dimension_types <- c("universal","annualized","other")
  if (!is.na(dimension_type) && !tolower(dimension_type) %in% valid_dimension_types)
    stop("❌ `dimension_type` must be one of: ", paste(valid_dimension_types, collapse = ", "))
  dimension_type <- if (is.na(dimension_type)) NA_character_ else tolower(dimension_type)

  log_entry <- data.frame(
    timestamp        = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
    file_name        = basename(path),
    file_path        = normalizePath(path),
    file_size_MB     = round(fi$size / 1e6, 2),
    canonical_table_id = canonical_table_id,
    dimension_type   = dimension_type,
    n_rows           = nrow(data),
    n_cols           = ncol(data),
    data_year        = log_metadata$data_year %||% NA,
    data_source      = ds_label,
    table_type       = table_type,
    data_description = log_metadata$data_description,
    dim_description  = dim_description %||% NA,
    user_note        = user_note,
    user             = Sys.info()[["user"]],
    stringsAsFactors = FALSE
  )

  if (file.exists(log_path)) {
    existing_log <- read.csv(log_path, stringsAsFactors = FALSE)
    match_idx <- which(existing_log$canonical_table_id == canonical_table_id)
    if (length(match_idx)) {
      existing_log[match_idx[1], ] <- log_entry
      write.csv(existing_log, log_path, row.names = FALSE)
      message("🔁 Existing log entry overwritten for: ", log_entry$file_name)
    } else {
      write.table(log_entry, log_path, append = TRUE, sep = ",",
                  row.names = FALSE, col.names = FALSE)
      message("📝 Log entry appended to: ", log_entry$file_name)
    }
  } else {
    write.table(log_entry, log_path, append = FALSE, sep = ",",
                row.names = FALSE, col.names = TRUE)
    message("📄 New log created: ", log_path)
  }

  invisible(NULL)
}


##


sql_schema_log <- function(data,
                           table_name,
                           data_year,
                           data_source,
                           user_note = NA,
                           dim_description = NULL,
                           max_char = 255,
                           primary_key = NULL,
                           foreign_keys = NULL,
                           canonical_table_id = NULL,
                           dimension_type = NULL) {

  # Validate required data_source
  valid_sources <- c("Assessment", "CDE", "Dashboard")
  if (is.null(data_source) || !(tolower(data_source) %in% tolower(valid_sources))) {
    stop("❌ `data_source` must be one of: ", paste(valid_sources, collapse = ", "))
  }

  data_source_label <- valid_sources[match(tolower(data_source), tolower(valid_sources))]
  data_source_dir <- tolower(data_source_label)

  # Check for fact or dim in user_note
  if (is.null(user_note) || !grepl("\\b(fact|dim)\\b", user_note, ignore.case = TRUE)) {
    stop("❌ Your `user_note` must clearly include either 'fact' or 'dim'.")
  }
  table_type <- tolower(stringr::str_extract(user_note, "\\b(fact|dim)\\b"))

  # Clean dim_description
  if (!is.null(dim_description)) {
    dim_description <- janitor::make_clean_names(dim_description)
  }

  # --- Validate primary key input ---
  data <- data.table::as.data.table(data)

  if (is.null(primary_key)) {
    stop("❌ You must supply a primary key for this table using primary_key.")
  }

  if (!all(primary_key %in% names(data))) {
    stop("❌ The specified primary key does not exist in the dataset: ",
         paste(setdiff(primary_key, names(data)), collapse = ", "))
  }

  # Optional but powerful: warn if not unique
  if (length(primary_key) == 1 && anyDuplicated(data[[primary_key]]) > 0) {
    stop("❌   The primary key column '", primary_key, "' contains duplicates.")
  }

  if (length(primary_key) > 1 && anyDuplicated(data[, ..primary_key, drop = FALSE]) > 0) {
    stop("❌ ️ The composite primary key (", paste(primary_key, collapse = ", "), ") contains duplicates.")
  }

  if (any(is.na(dplyr::select(data, all_of(primary_key))))) {
    stop("❌ ️ One or more of the primary key columns (",
            paste(primary_key, collapse = ", "), ") contain missing values.")
  }

  # --- Foreign key validation ---
  if (!is.null(foreign_keys)) {
    if (is.null(names(foreign_keys)) || any(names(foreign_keys) == "")) {
      stop("❌ foreign_keys must be a named character vector: c(col = 'ref_table')")
    }
    if (!all(names(foreign_keys) %in% names(data))) {
      stop("❌ Some foreign key columns are not found in the dataset: ",
           paste(setdiff(names(foreign_keys), names(data)), collapse = ", "))
    }
  }

  # Clean dim_description
  if (!is.null(dim_description)) {
    dim_description <- janitor::make_clean_names(dim_description)
  }

  # Build enhanced table name
  final_table_name <- paste0(
    table_name,
    if (!is.null(dim_description)) paste0("_", dim_description),
    "_", table_type
  )

  # Fallback if NULL
  user_note <- user_note %||% NA

  canonical_table_id <- canonical_table_id %||% table_name

  dimension_type <- dimension_type %||% NA

  valid_dimension_types <- c("universal", "annualized", "other")
  if (!is.null(dimension_type) && !is.na(dimension_type)) {
    if (!tolower(dimension_type) %in% valid_dimension_types) {
      stop("❌ `dimension_type` must be one of: ",
           paste(valid_dimension_types, collapse = ", "))
    }
    dimension_type <- tolower(dimension_type)
  }

  # Helper: Measure SQL type
  measure_sql_type <- function(x) {
    if (is.character(x)) {
      max_len <- max(nchar(x[!is.na(x)]), na.rm = TRUE)
      return(paste0("VARCHAR(", min(max_len, max_char), ")"))
    }
    else if (is.integer(x)) {
      max_digits <- max(nchar(abs(x[!is.na(x)])), na.rm = TRUE)
      return(paste0("INT(", max_digits, ")"))
    }
    else if (is.numeric(x)) {
      x <- na.omit(x)
      if (length(x) == 0) return("DECIMAL(10, 2)")

      formatted <- format(x, scientific = FALSE)
      whole_digits <- nchar(gsub("\\..*$", "", formatted))
      frac_digits <- nchar(gsub("^[^.]*\\.", "", formatted))
      frac_digits[!grepl("\\.", formatted)] <- 0

      p <- max(whole_digits + frac_digits, na.rm = TRUE)
      s <- max(frac_digits, na.rm = TRUE)
      return(paste0("DECIMAL(", p, ", ", s, ")"))
    }
    else if (is.logical(x)) {
      return("BOOLEAN")
    }
    else {
      return("TEXT")
    }
  }

  current_cols <- names(data)  # define just above the tibble

  # Build schema tibble
  schema <- tibble::tibble(
    table_name = final_table_name,
    column_name = names(data),
    sql_type = purrr::map_chr(data, measure_sql_type),
    null_flag = purrr::map_chr(data, ~ ifelse(any(is.na(.x)), "NULL", "NOT NULL")),
    is_primary_key = ifelse(current_cols %in% primary_key, "YES", "NO"),
    is_foreign_key = ifelse(current_cols %in% names(foreign_keys), "YES", "NO"),
    foreign_key_ref = ifelse(
      current_cols %in% names(foreign_keys),
      paste0(foreign_keys[current_cols], "(", current_cols, ")"),
      NA_character_),
    primary_key_group = ifelse(
      current_cols %in% primary_key,
      paste(primary_key, collapse = " + "),
      NA_character_),
    dimension_type = dimension_type,
    data_year = data_year,
    data_source = data_source_label,
    dim_description = dim_description %||% NA,
    canonical_table_id = canonical_table_id,
    user_note = user_note,
    timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
    user = Sys.info()[["user"]]
  ) %>%
    dplyr::select(table_name, column_name, sql_type, null_flag,
                  is_primary_key, is_foreign_key, foreign_key_ref,
                  primary_key_group, dimension_type, data_year, data_source,
                  canonical_table_id, dim_description, user_note,
                  timestamp, user)

  # Define shared base directory for schema files
  base_dir <- "T:/Data Warehouse"

  # Define subpaths using base_dir
  schema_log_dir <- file.path(base_dir, "schema_files")
  primary_log_path <- file.path(schema_log_dir, "primary_sql_schema_log.csv")

  # Save to primary log
  if (!dir.exists(dirname(primary_log_path))) {
    dir.create(dirname(primary_log_path), recursive = TRUE)
  }

  if (file.exists(primary_log_path)) {
    existing_log <- readr::read_csv(primary_log_path, show_col_types = FALSE)

    # Convert timestamp to character for consistent binding
    existing_log$timestamp <- as.character(existing_log$timestamp)

    schema <- dplyr::bind_rows(
      dplyr::filter(existing_log, canonical_table_id != !!canonical_table_id),
      schema
    )
  }

  readr::write_csv(schema, primary_log_path)
  message("🧾 Primary schema log updated at: ", primary_log_path)

  # Save individual schema file
  individual_path <- file.path(
    schema_log_dir,
    as.character(data_year),
    data_source_dir,
    table_type,
    paste0(final_table_name, "_schema.csv")
  )
  if (!dir.exists(dirname(individual_path))) {
    dir.create(dirname(individual_path), recursive = TRUE)
  }
  # Filter just the current table’s schema
  current_schema <- dplyr::filter(schema, table_name == !!final_table_name)
  readr::write_csv(current_schema, individual_path)
  message("📄 Table-level schema saved at: ", individual_path)

  invisible(schema)
}

###

# this is a wrapper function to combine safe_fwrite & sql_schema_log together!
export_with_schema <- function(data,
                               path = NULL,
                               table_name,
                               data_year,
                               data_source,
                               data_description,
                               dim_description = NULL,
                               user_note,
                               char_cols = c("cds", "county_code", "district_code", "school_code"),
                               compress = FALSE,
                               n_check = 6,
                               export_log_path = "T:/Data Warehouse/export_log.csv",
                               max_char = 255,
                               primary_key = NULL,
                               foreign_keys = NULL,
                               canonical_table_id = NULL,
                               dimension_type = NULL) {

  validate_sql_identifiers(table_name, type = "table")

  validate_sql_identifiers(names(data), type = "column")

  # Run safe_fwrite
  safe_fwrite(
    data = data,
    path = path,
    char_cols = char_cols,
    compress = compress,
    n_check = n_check,
    data_year = data_year,
    data_source = data_source,
    data_description = data_description,
    dim_description = dim_description,
    user_note = user_note,
    table_name = table_name,
    log_path = export_log_path,
    canonical_table_id = canonical_table_id,
    dimension_type = dimension_type
  )

  # Run sql_schema_log
  sql_schema_log(
    data = data,
    table_name = table_name,
    data_year = data_year,
    data_source = data_source,
    user_note = user_note,
    dim_description = dim_description,
    max_char = max_char,
    primary_key = primary_key,
    foreign_keys = foreign_keys,
    canonical_table_id = canonical_table_id,
    dimension_type = dimension_type
  )

  invisible(NULL)
}

# User friendly notes to also include with export_with_schema

# ==============================================================================
# export_with_schema(): Export Data with Metadata + SQL Schema Logging
# ==============================================================================

# DESCRIPTION:
# This wrapper function automates two essential reproducibility steps:
# 1. ✅ Writes a CSV file and logs key metadata to `export_log.csv` via `safe_fwrite()`
# 2. 🧾 Documents the data structure in a SQL-style schema via `sql_schema_log()`
# 3. 🛑 Validates table and column names for SQL safety before writing:
#    ❌ Stops if any names contain spaces, special characters, or exceed 63 characters

#
# Together, these ensure:
# - Clean export of your dataset with key character columns preserved
# - Centralized metadata tracking (file name, size, rows, year, source, etc.)
# - Automatically generated SQL-friendly schema with column types, null flags, and key roles
# - Tracks both primary and foreign keys for SQL clarity and joinability
# - Organized folder structure for schema files based on data source and table type
# - Optional metadata like `dimension_type` supports warehouse organization
# - Enforces SQL-safe naming for tables and columns (alphanumeric or underscores; max 63 chars)

# ------------------------------------------------------------------------------
# ⬇️ export_with_schema(data, path, table_name, data_year, data_source, data_description, user_note, ...)
#
# REQUIRED ARGS:
# - data:              The data frame to export
# - path:              File path to save the CSV (optional if using naming convention)
# - table_name:        Short name for table (used in schema filenames and log)
#                      ✅ Must be SQL-safe: letters, numbers, or underscores; max 63 characters
# - data_year:         Numeric year (e.g., 2024)
# - data_source:       One of: "Assessment", "CDE", or "Dashboard" (case-insensitive)
#                      ✅ Stored as Title Case in logs; lowercase version used for folders
# - data_description:  One-line summary of what the dataset contains
# - user_note:         Must include either "fact" or "dim" — drives schema folder and suffix
# - primary_key:       Vector of one or more column names that define the primary key
#                      ✅ All column names must be SQL-safe and ≤ 63 characters

# OPTIONAL ARGS:
# - foreign_keys:      Named character vector of foreign key references
#                      Format: c(col_name = "ref_table")
#                      ✅ Example: c(school_code = "school_dim", grade = "grade_dim")
#                      ✅ Recommended: Use `fk_dim("table_name")` to auto-append `_dim`
#                         ➤ Example: c(grade = fk_dim("grade_levels"))
# - dim_description:   Optional suffix to differentiate similar tables (e.g., "by_grade")
# - dimension_type:    One of: "universal", "annualized", or "other"
#                      Describes how the table behaves over time
#                      Used for organizing warehouse-level schema files
# - canonical_table_id: Optional ID used to link renamed versions of the same table
#                       Defaults to `table_name` if not supplied
# - char_cols:         Columns to coerce to character (default: CDS-style IDs)
# - compress:          If TRUE, compresses the CSV output with `.gz` (default: FALSE)
# - n_check:           Number of rows to preview after writing (default: 6)
# - export_log_path:   Where to write the export metadata log (default: "export_log.csv")
# - max_char:          Max VARCHAR length used for character fields in SQL schema (default: 255)

# ------------------------------------------------------------------------------
# ✅ Best Practices:
# - Include "fact" or "dim" in your `user_note` to guide schema logging and file suffix
# - Use `primary_key` for the actual composite key that uniquely defines each row
# - Use `foreign_keys` for linking to external dimension tables — even if overlapping with PK
# - Use `fk_dim("table_name")` to help enforce `_dim` naming convention on foreign key refs
# - Use `data_description` to help IT and future-you understand file contents
# - Use `dimension_type` to label tables as "universal", "annualized", or "other" for later warehouse promotion
# - Avoid renaming key columns after deriving them from a larger source — consistency enables auto-logging
# - Call this function once per exported file to generate schema + metadata together

# ------------------------------------------------------------------------------
# Example:
# export_with_schema(
#   data = sbac24_area_long,
#   table_name = "sbac24_area",
#   data_year = 2024,
#   data_source = "Assessment",
#   data_description = "SBAC area-level performance scores, disaggregated by student group",
#   user_note = "2024 SBAC area scores (ELA has 4 areas, math has 3), fact table",
#   primary_key = c("cds", "year", "test", "grade", "demo_group", "area"),
#   foreign_keys = c(
#     cds = fk_dim("school"),
#     grade = fk_dim("grade_levels"),
#     demo_group = fk_dim("group"),
#     area = fk_dim("area")
#   ),
#   dimension_type = "annualized"
# )


# Code for identifying composite primary keys

validate_primary_key <- function(data, key_cols, sample_n = 10000, full_run = FALSE, seed = 1234, show_examples = 10) {

  data <- as.data.frame(data)  # ensure base R compatibility

  # Column check
  if (!all(key_cols %in% names(data))) {
    stop("❌ One or more key columns are not present in the dataset: ",
         paste(setdiff(key_cols, names(data)), collapse = ", "))
  }

  total_rows <- nrow(data)

  # Sampling logic
  if (!full_run && total_rows > sample_n) {

    set.seed(seed)

    idx  <- sample.int(total_rows, sample_n)
    data <- data[idx, , drop = FALSE]

    cat("📊 Sampled ", sample_n, " of ", total_rows, " rows for primary key validation.\n")
    cat("🔍 To check the entire dataset, use: full_run = TRUE\n")
  } else {

    cat("📊 Checking all ", total_rows, " rows for primary key validation...\n")
  }

  # 3) NA check in key columns
  na_counts <- vapply(key_cols, function(k) sum(is.na(data[[k]])), integer(1))
  if (any(na_counts > 0)) {
    message("⚠️ NAs in key columns:")
    print(stats::setNames(as.integer(na_counts), key_cols))
  }

  # 4) Duplicate check
  dup_flag <- duplicated(data[, key_cols, drop = FALSE])
  dup_n    <- sum(dup_flag)

  if (dup_n == 0 && all(na_counts == 0)) {
    message("\033[32m✅ These columns [", paste(key_cols, collapse = ", "), "] comprise the primary key",
            if (!full_run && total_rows > sample_n) " (within this sample)", ".", "\033[0m")
    return(invisible(list(pass = TRUE, duplicates = 0, na = na_counts)))
  }

  # 5) Report duplicates (with examples)
  if (dup_n > 0) {
    message("\033[31m❌ ", dup_n, " duplicate row(s) by [", paste(key_cols, collapse = ", "), "].\033[0m")
    dups <- data[dup_flag, key_cols, drop = FALSE]
    if (nrow(dups) > show_examples) dups <- head(dups, show_examples)
    message("🔎 Example offending keys:")
    print(unique(dups))
  }

  invisible(list(pass = FALSE, duplicates = dup_n, na = na_counts))
}


# Code for checking CDS duplicates
################################
# Check for duplicate CDS codes

# ltel24_entities_fact %>% tabyl(org_level) # n tells how many rows each dim should have
#
# nrow(ltel24_county_dim) # county + state
# nrow(ltel24_district_dim) # district codes
# nrow(ltel24_school_dim) # school codes
# # 10692 > 10633 from tabyl, issue with duplicate CDS codes for schools
#
# cds_name_counts <- ltel24_school_dim %>%
#   group_by(cds) %>%
#   summarise(n_names = n_distinct(school_name), .groups = "drop") %>%
#   mutate(dup_cds = as.integer(n_names > 1)) # creates dummy flag for any duplicated CDS codes
#
# cds_name_counts %>% tabyl(dup_cds) # 44 codes get reused
#
# print(cds_name_counts %>% filter(dup_cds == 1))
#
# cds_name_counts %>% filter(n_names > 1) %>% summarise(sum(n_names)) # 141 rows have duplicated codes
#
# orig_school_length <- nrow(ltel24_school_dim)
#
# ltel24_school_dim <- ltel24_school_dim %>%
#   anti_join(cds_name_counts %>% filter(dup_cds == 1) %>% select(cds, dup_cds), by = "cds") # anti_join cds_dup to school_dim
# # what this does is remove the duplicated cds codes from school dim
# # rows are still kept in file and aggregated up by CDE, but not sliceable in school list
#
# new_school_length <- nrow(ltel24_school_dim)
#
# orig_school_length - new_school_length # 141 rows dropped
# this should match sum of n_names from above


# Helper function for finding likely dim table objects
############

get_dim_objects <- function(keywords = c("dim", "entit", "group", "label")) { # these are keywords it will check against
  all_objs <- ls(envir = .GlobalEnv)
  pattern <- paste(keywords, collapse = "|")

  dim_objs <- stringr::str_subset(all_objs, pattern)
  dim_objs <- setdiff(dim_objs, c("get_dim_objects", "clear_dim_objects",
                                  "assessment_files_group_labeling",
                                  "cde_files_group_labeling",
                                  "dashboard_files_group_labeling",
                                  "validate_group_mapping_tabyl"))  # Exclude known functions

  if (length(dim_objs) > 0) {
    cli::cli_h1("🗂️  Dimension-like Objects Found")
    cli::cli_ul(dim_objs)
  } else {
    cli::cli_alert_info("No matching dimension-like objects found.")
  }

  return(invisible(dim_objs))
}

#ltel19_dims <- get_dim_objects() # example

##############################


# Helper function cleaning old dims from working environment
###############################

# after exporting dims, clean old dims out of working environment
# this is so you can run a new version of get_dim_objects without worry

clear_dim_objects <- function(base_string = NULL, dry_run = TRUE,
                              keywords = c("dim", "entit", "group", "label")) { # these are same keywords it will check against
  if (is.null(base_string)) {
    cli::cli_alert_danger("You must supply a `base_string` to filter (e.g., 'ltel19', 'sbac24').")
    return(invisible(NULL))
  }

  all_objs <- ls(envir = .GlobalEnv)
  pattern <- paste(keywords, collapse = "|")
  dim_objs <- stringr::str_subset(all_objs, pattern)
  matching_objs <- stringr::str_subset(dim_objs, paste0("^", base_string))

  if (length(matching_objs) == 0) {
    cli::cli_alert_info("No matching dimension-like objects found for {.val {base_string}}.")
    return(invisible(NULL))
  }

  cli::cli_h1("Objects to be removed:")
  cli::cli_ul(matching_objs)

  if (!dry_run) {
    rm(list = matching_objs, envir = .GlobalEnv)
    cli::cli_alert_success("Removed {.val {length(matching_objs)}} objects.")
  } else {
    cli::cli_alert_info("This was a dry run. Set {.code dry_run = FALSE} to remove.")
  }

  invisible(matching_objs)
}

# clear_dim_objects() # you will need to specify a base string

###########################

# Verify padded cds

# checks if pad_cds_codes worked after moving data from real machine to VM through Drive
######################
verify_padded_cds <- function(df, drop_parts = TRUE) {
  expected_lengths <- c(
    county_code = 2,
    district_code = 5,
    school_code = 7,
    cds = 14
  )

  issues <- purrr::imap(expected_lengths, function(width, col) {
    if (!col %in% names(df)) {
      return(glue::glue("❌ Column '{col}' is missing."))
    }

    values <- df[[col]]
    if (!is.character(values)) {
      return(glue::glue("❌ Column '{col}' is not character (is {typeof(values)})."))
    }

    bad_n <- sum(nchar(values) != width, na.rm = TRUE)
    if (bad_n > 0) {
      return(glue::glue("⚠️ Column '{col}' has {bad_n} rows with incorrect length (expected {width})."))
    }

    NULL
  }) %>% purrr::compact()

  if (length(issues) > 0) {
    cli::cli_alert_danger("CDS post-padding validation failed:")
    cli::cli_bullets(issues)
    return(FALSE)
  }

  cli::cli_alert_success("✅ All CDS-related columns are correctly padded and validated.")

  # Drop component columns if validation succeeded and user requested cleanup
  if (drop_parts) {
    df <- df %>% select(-county_code, -district_code, -school_code)
    cli::cli_alert_info("📦 Dropped component columns: county_code, district_code, school_code")
  }

  return(df)
}

###########

# Checks variable names between two datasets and displays differences in variable names
############
compare_variable_names <- function(df1, df2) {
  name1 <- deparse(substitute(df1))
  name2 <- deparse(substitute(df2))

  cols1 <- names(df1)
  cols2 <- names(df2)

  identical_cols <- identical(cols1, cols2)
  diff_1_to_2 <- setdiff(cols1, cols2)
  diff_2_to_1 <- setdiff(cols2, cols1)

  # Create and print the summary tibble
  summary_df <- tibble(
    Comparison = paste(name1, "vs", name2),
    Identical = identical_cols
  )

  # Print differences as separate messages
  if (length(diff_1_to_2) > 0) {
    cat("\nIn", name1, "but not in", name2, ":\n", paste(diff_1_to_2, collapse = ", "), "\n")
  } else {
    cat("\nNo variables found in", name1, "that are missing from", name2, "\n")
  }

  if (length(diff_2_to_1) > 0) {
    cat("\nIn", name2, "but not in", name1, ":\n", paste(diff_2_to_1, collapse = ", "), "\n")
  } else {
    cat("\nNo variables found in", name2, "that are missing from", name1, "\n")
  }
}

# compare_variable_names(df1 = grad23, df2 = grad24)

# Verifying that suppression flag has worked correctly
####
# Function for verifying suppression
check_suppression_dependency <- function(df, numeric_cols, trigger_col = "cohort_students", return_rows = FALSE) {
  # 1. All suppressed rows: any "*" in numeric columns
  suppressed_rows <- df %>%
    filter(if_any(all_of(numeric_cols), ~ str_detect(.x, "\\*")))

  # 2. Check if ALL suppressed rows have "*" in trigger_col
  all_trigger_suppressed <- all(suppressed_rows[[trigger_col]] == "*")

  # 3. Only trigger_col is suppressed
  only_trigger_suppressed_rows <- df %>%
    filter(.data[[trigger_col]] == "*") %>%
    filter(if_all(setdiff(numeric_cols, trigger_col), ~ .x != "*"))

  only_trigger_suppressed <- nrow(only_trigger_suppressed_rows)

  # 4. Other suppression without trigger_col suppressed
  others_suppressed_but_not_trigger_rows <- df %>%
    filter(.data[[trigger_col]] != "*") %>%
    filter(if_any(setdiff(numeric_cols, trigger_col), ~ str_detect(.x, "\\*")))

  others_suppressed_but_not_trigger <- nrow(others_suppressed_but_not_trigger_rows)

  # Base summary
  summary_tbl <- tibble::tibble(
    all_trigger_suppressed_in_suppressed_rows = all_trigger_suppressed,
    only_trigger_suppressed_rows_found = only_trigger_suppressed,
    other_suppressed_when_trigger_not = others_suppressed_but_not_trigger
  )

  # Return with or without attached rows
  if (return_rows) {
    return(list(
      summary = summary_tbl,
      suppressed_rows = suppressed_rows,
      only_trigger_suppressed_rows = only_trigger_suppressed_rows,
      others_suppressed_but_not_trigger_rows = others_suppressed_but_not_trigger_rows
    ))
  } else {
    return(summary_tbl)
  }
}

# Function to create demo group labels, codes and groups which can be used for dim construction and exporting
####
#USE THIS FOR CDE FILES THAT ARE NOT DASHBOARD OR ASSESSMENT (ELPAC, CAST, SBAC)
cde_files_group_labeling <- function(df, var_names, output_names) {
  if (length(var_names) != length(output_names)) {
    stop("You must have the same number of variables as outputs")
  }

  missing_cols <- setdiff(var_names, names(df))
  if (length(missing_cols) > 0) {
    stop(paste("Missing columns in dataframe:", paste(missing_cols, collapse = ", ")))
  }

  # ---- Internal helper: Only prompt if duplicates are also in the data ----
  resolve_duplicates_interactively <- function(classification_map, df, var_names) {
    all_values <- unlist(lapply(classification_map, function(x) x$values))
    dupes <- unique(all_values[duplicated(all_values)])

    # Get all values in the data across the relevant columns
    data_values <- unique(unlist(as.data.frame(df)[, var_names]))

    # Only keep duplicated values that also appear in the data
    relevant_dupes <- intersect(dupes, data_values)
    if (length(relevant_dupes) == 0) return(classification_map)

    message("Duplicate classification values found in your data. Please choose which label to apply:")

    resolved_map <- list()

    for (dup in relevant_dupes) {
      matching_entries <- Filter(function(x) dup %in% x$values, classification_map)

      cat("\nValue:", dup, "\n")
      for (i in seq_along(matching_entries)) {
        cat(i, "-> label:", matching_entries[[i]]$label,
            "| num:", matching_entries[[i]]$num,
            "| group_num:", matching_entries[[i]]$group_num,
            "| group:", matching_entries[[i]]$group, "\n")
      }

      choice <- as.integer(readline(prompt = "Select the number of the preferred option: "))
      selected <- matching_entries[[choice]]
      selected$values <- dup  # Keep only selected value

      # Remove the value from all other entries in the full map
      classification_map <- lapply(classification_map, function(x) {
        x$values <- setdiff(x$values, dup)
        x
      })

      # Add selected entry back
      resolved_map <- append(resolved_map, list(selected))
    }

    # Keep all other non-duplicate or unresolved entries
    unique_map <- Filter(function(x) all(!x$values %in% relevant_dupes), classification_map)
    final_map <- append(unique_map, resolved_map)

    return(final_map)
  }

  # Sample hardcoded classification map
  classification_map <- list(
    list(values = c("RB", "RE_B"), label = "African American", num = 1, group_num = 1, group = "Race"),
    list(values = c("RI", "RE_I"), label = "American Indian or Alaska Native", num = 2, group_num = 1, group = "Race"),
    list(values = c("RA", "RE_A"), label = "Asian", num = 3, group_num = 1, group = "Race"),
    list(values = c("RF", "RE_F"), label = "Filipino", num = 4, group_num = 1, group = "Race"),
    list(values = c("RH", "RE_H"), label = "Hispanic or Latino", num = 5, group_num = 1, group = "Race"),
    list(values = c("RP", "RE_P"), label = "Pacific Islander", num = 6, group_num = 1, group = "Race"),
    list(values = c("RT", "RE_T"), label = "Two or More Races", num = 7, group_num = 1, group = "Race"),
    list(values = c("RW", "RE_W"), label = "White", num = 8, group_num = 1, group = "Race"),
    list(values = c("RD", "RE_D"), label = "Race Not Reported", num = 9, group_num = 1, group = "Race"),
    list(values = c("GRKN", "GRK"), label = "Kindergarten", num = 10, group_num = 2, group = "Grade"),
    list(values = c("GR13"), label = "Grades 1-3", num = 11, group_num = 2, group = "Grade"),
    list(values = c("GS_46", "GR46"), label = "Grades 4-6", num = 12, group_num = 2, group = "Grade"),
    list(values = c("GR78"), label = "Grades 7-8", num = 13, group_num = 2, group = "Grade"),
    list(values = c("GRK8"), label = "Grades K-8", num = 14, group_num = 2, group = "Grade"),
    list(values = c("GS_912", "GR912"), label = "Grades 9-12", num = 15, group_num = 2, group = "Grade"),
    list(values = c("GRTKKN"), label = "Grades TK-K", num = 16, group_num = 2, group = "Grade"),
    list(values = c("GRTK8"), label = "Grades TK-8", num = 17, group_num = 2, group = "Grade"),
    list(values = c("GRTK"), label = "Grade TK", num = 18, group_num = 2, group = "Grade"),
    list(values = c("GR04"), label = "Grade 4", num = 19, group_num = 2, group = "Grade"),
    list(values = c("GR08"), label = "Grade 8", num = 20, group_num = 2, group = "Grade"),
    list(values = c("GR09"), label = "Grade 9", num = 21, group_num = 2, group = "Grade"),
    list(values = c("GR10"), label = "Grade 10", num = 22, group_num = 2, group = "Grade"),
    list(values = c("GR11"), label = "Grade 11", num = 23, group_num = 2, group = "Grade"),
    list(values = c("GR12"), label = "Grade 12", num = 24, group_num = 2, group = "Grade"),
    list(values = c("GS_PS3"), label = "Grades PreK-3", num = 25, group_num = 2, group = "Grade"),
    list(values = c("AR_03"), label = "Children K-12 who are 0-3", num = 26, group_num = 3, group = "Age Range"),
    list(values = c("AR_0418"), label = "Children K-12 who are 4-18", num = 27, group_num = 3, group = "Age Range"),
    list(values = c("AR_1922"), label = "Continuing Students K-12 who are 19-12", num = 28, group_num = 3, group = "Age Range"),
    list(values = c("AR_2329"), label = "Adults K-12 who are 23-29", num = 29, group_num = 3, group = "Age Range"),
    list(values = c("AR_3039"), label = "Adults K-12 who are 30-39", num = 30, group_num = 3, group = "Age Range"),
    list(values = c("AR_4049"), label = "Adults K-12 who are 40-49", num = 31, group_num = 3, group = "Age Range"),
    list(values = c("AR_50P"), label = "Adults K-12 who are 50 plus", num = 32, group_num = 3, group = "Age Range"),
    list(values = c("GN_F", "GF"), label = "Female", num = 33, group_num = 4, group = "Gender"),
    list(values = c("GN_M", "GM"), label = "Male", num = 34, group_num = 4, group = "Gender"),
    list(values = c("GN_X", "GX", "GN"), label = "Non-Binary", num = 35, group_num = 4, group = "Gender"),
    list(values = c("GN_Z", "GX", "GZ"), label = "Gender Missing", num = 36, group_num = 4, group = "Gender"),
    list(values = c("SG_EL", "SE", "ELAS_EL"), label = "English Learner", num = 37, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("SG_DS", "SD"), label = "Students with Disabilities", num = 38, group_num = 5, group = "Student Subgroup"),
    list(values = c("SG_SD", "SS"), label = "Socioeconomically Disadvantaged", num = 39, group_num = 5, group = "Student Subgroup"),
    list(values = c("SG_MG", "SM"), label = "Migrant Youth", num = 40, group_num = 5, group = "Student Subgroup"),
    list(values = c("SG_FS", "SF"), label = "Foster Youth", num = 41, group_num = 5, group = "Student Subgroup"),
    list(values = c("SG_HM", "SH"), label = "Homeless Youth", num = 42, group_num = 5, group = "Student Subgroup"),
    list(values = c("S5"), label = "Student with a 504 Accommodation Plan", num = 43, group_num = 5, group = "Student Subgroup"),
    list(values = c("HUYN"), label = "Not Homeless Unaccompanied Youth", num = 44, group_num = 5, group = "Student Subgroup"),
    list(values = c("HUYY"), label = "Homeless Unaccompanied Youth", num = 45, group_num = 5, group = "Student Subgroup"),
    list(values = c("EL_Y"), label = "Is an English Learner", num = 46, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("EL_Y"), label = "Is Not an English Learner", num = 47, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("CAY"), label = "Is Chronically Absent", num = 48, group_num = 5, group = "Student Subgroup"),
    list(values = c("CAN"), label = "Is Not Chronically Absent", num = 49, group_num = 5, group = "Student Subgroup"),
    list(values = c("TA"), label = "Total Number of Students", num = 50, group_num = 6, group = "Total Number of Students"),
    list(values = c("ELAS_ADEL"), label = "Adult English Learner", num = 51, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("ELAS_EO"), label = "English Only", num = 52, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("ELAS_IFEP"), label = "Initial Fluent English Proficient", num = 53, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("ELAS_MISS"), label = "EL Status Missing", num = 54, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("ELAS_RFEP"), label = "Reclassified Fluent English Proficient", num = 55, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("ELAS_TBD"), label = "English Status TBD", num = 56, group_num = 8, group = "English Language Acquisition Status"),
    list(values = c("DC_AUT"), label = "Autism", num = 57, group_num = 7, group = "Disability Category"),
    list(values = c("DC_DB"), label = "Deaf Blindedness", num = 58, group_num = 7, group = "Disability Category"),
    list(values = c("DC_DFHI"), label = "Deaf/Hearing Impairment", num = 59, group_num = 7, group = "Disability Category"),
    list(values = c("DC_ED"), label = "Emotional Disturbance", num = 60, group_num = 7, group = "Disability Category"),
    list(values = c("DC_EMD"), label = "Established Medical Disability", num = 61, group_num = 7, group = "Disability Category"),
    list(values = c("DC_HH"), label = "Hard of Hearing", num = 62, group_num = 7, group = "Disability Category"),
    list(values = c("DC_ID"), label = "Intellectual Disability", num = 63, group_num = 7, group = "Disability Category"),
    list(values = c("DC_MD"), label = "Multiple Disabilities", num = 64, group_num = 7, group = "Disability Category"),
    list(values = c("DC_OHI"), label = "Other Health Impairment", num = 65, group_num = 7, group = "Disability Category"),
    list(values = c("DC_OI"), label = "Orthopedic Health Impairment", num = 66, group_num = 7, group = "Disability Category"),
    list(values = c("DC_SLD"), label = "Specific Learning Disability", num = 67, group_num = 7, group = "Disability Category"),
    list(values = c("DC_SLI"), label = "Speech or Language Impairment", num = 68, group_num = 7, group = "Disability Category"),
    list(values = c("DC_TBI"), label = "Traumatic Brain Injury", num = 69, group_num = 7, group = "Disability Category"),
    list(values = c("DC_VI"), label = "Visual Impairment", num = 70, group_num = 7, group = "Disability Category"),
    list(values = c("AR_35"), label = "Ages 3-5", num = 71, group_num = 3, group = "Age Range"),
    list(values = c("AR_612"), label = "Ages 6-12", num = 72, group_num = 3, group = "Age Range"),
    list(values = c("AR_1318"), label = "Ages 13-18", num = 73, group_num = 3, group = "Age Range"),
    list(values = c("AR_19P"), label = "Ages 19 plus", num = 74, group_num = 3, group = "Age Range")
  )

  # ---- Resolve only relevant duplicates ----
  classification_map <- resolve_duplicates_interactively(classification_map, df, var_names)

  # ---- Apply classification across each column ----
  for (i in seq_along(var_names)) {
    var <- var_names[i]
    prefix <- output_names[i]

    label_col <- paste0(prefix, "_label")
    num_col  <- paste0(prefix, "_num")
    group_num_col <- paste0(prefix, "_group_num")
    group_col <- paste0(prefix, "_group")

    df[[label_col]] <- NA_character_
    df[[num_col]]  <- NA_real_
    df[[group_num_col]] <- NA_real_
    df[[group_col]] <- NA_character_

    current_values <- df[[var]]

    for (entry in classification_map) {
      matched <- ifelse(current_values %in% entry$values, TRUE, FALSE)

      if (any(matched, na.rm = TRUE)) {
        df[[label_col]] <- ifelse(is.na(df[[label_col]]) & matched, entry$label, df[[label_col]])
        df[[num_col]]  <- ifelse(is.na(df[[num_col]])  & matched, entry$num,  df[[num_col]])
        df[[group_num_col]] <- ifelse(is.na(df[[group_num_col]]) & matched, entry$group_num, df[[group_num_col]])
        df[[group_col]] <- ifelse(is.na(df[[group_col]]) & matched, entry$group, df[[group_col]])
      }
    }
  }

  # ---- Final validation: Stop if unmapped values are found ----
  for (i in seq_along(var_names)) {
    var <- var_names[i]
    prefix <- output_names[i]
    label_col <- paste0(prefix, "_label")

    # Only consider rows where input is not NA but label is NA
    unmatched_vals <- unique(df[[var]][is.na(df[[label_col]]) & !is.na(df[[var]])])

    if (length(unmatched_vals) > 0) {
      stop(
        "❌ The following ", length(unmatched_vals), " value(s) in column '", var,
        "' could not be mapped:\n→ ",
        paste(unmatched_vals, collapse = ", "),
        "\nPlease update the classification map to include these values."
      )
    }
  }

  return(df)
}

  # setwd("T:/CDE data releases/Absenteeism Data (2019-present)/2023-24 data")
  # chronic24 <- fread(paste0("chronicabsenteeism24.txt"),
  #                    sep = ",",
  #                    colClasses = c("County Code" = "character",
  #                                   "District Code" = "character",
  #                                   "School Code" = "character")) # need these to be character for concat
  # #Example usage
  # chronic24 <- cde_files_group_labeling(chronic24,
  #                                         var_names = c("Reporting Category"),
  #                                         output_names = c("student_groups"))



#USE THIS ONLY FOR DASHBOARD FILES
dashboard_files_group_labeling <- function(df, var_names, output_names) {
  if (length(var_names) != length(output_names)) {
    stop("You must have the same number of variables as outputs")
  }

  missing_cols <- setdiff(var_names, names(df))
  if (length(missing_cols) > 0) {
    stop(paste("Missing columns in dataframe:", paste(missing_cols, collapse = ", ")))
  }

  # ---- Internal helper: Only prompt if duplicates are also in the data ----
  resolve_duplicates_interactively <- function(classification_map, df, var_names) {
    all_values <- unlist(lapply(classification_map, function(x) x$values))
    dupes <- unique(all_values[duplicated(all_values)])

    # Get all values in the data across the relevant columns
    data_values <- unique(unlist(as.data.frame(df)[, var_names]))

    # Only keep duplicated values that also appear in the data
    relevant_dupes <- intersect(dupes, data_values)
    if (length(relevant_dupes) == 0) return(classification_map)

    message("Duplicate classification values found in your data. Please choose which label to apply:")

    resolved_map <- list()

    for (dup in relevant_dupes) {
      matching_entries <- Filter(function(x) dup %in% x$values, classification_map)

      cat("\nValue:", dup, "\n")
      for (i in seq_along(matching_entries)) {
        cat(i, "-> label:", matching_entries[[i]]$label,
            "| num:", matching_entries[[i]]$num,
            "| group_num:", matching_entries[[i]]$group_num,
            "| group:", matching_entries[[i]]$group, "\n")
      }

      choice <- as.integer(readline(prompt = "Select the number of the preferred option: "))
      selected <- matching_entries[[choice]]
      selected$values <- dup  # Keep only selected value

      # Remove the value from all other entries in the full map
      classification_map <- lapply(classification_map, function(x) {
        x$values <- setdiff(x$values, dup)
        x
      })

      # Add selected entry back
      resolved_map <- append(resolved_map, list(selected))
    }

    # Keep all other non-duplicate or unresolved entries
    unique_map <- Filter(function(x) all(!x$values %in% relevant_dupes), classification_map)
    final_map <- append(unique_map, resolved_map)

    return(final_map)
  }

  # Sample hardcoded classification map
  classification_map <- list(
    list(values = c("AA"), label = "Black/African American", num = 1, group_num = 1, group = "Race"),
    list(values = c("AI"), label = "American Indian or Alaska Native", num = 2, group_num = 1, group = "Race"),
    list(values = c("AS"), label = "Asian", num = 3, group_num = 1, group = "Race"),
    list(values = c("FI"), label = "Filipino", num = 4, group_num = 1, group = "Race"),
    list(values = c("HI"), label = "Hispanic", num = 5, group_num = 1, group = "Race"),
    list(values = c("PI"), label = "Pacific Islander", num = 6, group_num = 1, group = "Race"),
    list(values = c("MR"), label = "Multiple Races/Two or More", num = 7, group_num = 1, group = "Race"),
    list(values = c("WH"), label = "White", num = 8, group_num = 1, group = "Race"),
    list(values = c("SED"), label = "Socioeconomically Disadvantaged", num = 9, group_num = 2, group = "Student Subgroup"),
    list(values = c("SWD"), label = "Students with Disabilities", num = 10, group_num = 2, group = "Student Subgroup"),
    list(values = c("FOS"), label = "Foster Youth", num = 11, group_num = 2, group = "Student Subgroup"),
    list(values = c("HOM"), label = "Homeless Youth", num = 12, group_num = 2, group = "Student Subgroup"),
    list(values = c("RFP"), label = "Recently Reclassified Fluent-English Proficient Only", num = 13, group_num = 4, group = "English Language Acquisition Status"),
    list(values = c("LTEL"), label = "Long-Term English Learner", num = 14, group_num = 2, group = "Student Subgroup"),
    list(values = c("SBA"), label = "Students Who Took SBAC", num = 15, group_num = 3, group = "Test Taken"),
    list(values = c("CAA"), label = "Students Who Took CAA", num = 16, group_num = 3, group = "Test Taken"),
    list(values = c("CAST"), label = "Students Who Took CAST", num = 17, group_num = 3, group = "Test Taken"),
    list(values = c("EL"), label = "English Learner", num = 18, group_num = 4, group = "English Language Acquisition Status"),
    list(values = c("ELO"), label = "English Learners Only", num = 19, group_num = 4, group = "English Language Acquisition Status"),
    list(values = c("EO"), label = "English Only", num = 20, group_num = 4, group = "English Language Acquisition Status"),
    list(values = c("ALL"), label = "All Students", num = 21, group_num = 5, group = "All Students")
  )

  # ---- Resolve only relevant duplicates ----
  classification_map <- resolve_duplicates_interactively(classification_map, df, var_names)

  # ---- Apply classification across each column ----
  for (i in seq_along(var_names)) {
    var <- var_names[i]
    prefix <- output_names[i]

    label_col <- paste0(prefix, "_label")
    num_col  <- paste0(prefix, "_num")
    group_num_col <- paste0(prefix, "_group_num")
    group_col <- paste0(prefix, "_group")

    df[[label_col]] <- NA_character_
    df[[num_col]]  <- NA_real_
    df[[group_num_col]] <- NA_real_
    df[[group_col]] <- NA_character_

    current_values <- df[[var]]

    for (entry in classification_map) {
      matched <- ifelse(current_values %in% entry$values, TRUE, FALSE)

      if (any(matched, na.rm = TRUE)) {
        df[[label_col]] <- ifelse(is.na(df[[label_col]]) & matched, entry$label, df[[label_col]])
        df[[num_col]]  <- ifelse(is.na(df[[num_col]])  & matched, entry$num,  df[[num_col]])
        df[[group_num_col]] <- ifelse(is.na(df[[group_num_col]]) & matched, entry$group_num, df[[group_num_col]])
        df[[group_col]] <- ifelse(is.na(df[[group_col]]) & matched, entry$group, df[[group_col]])
      }
    }
  }

  # ---- Final validation: Stop if unmapped values are found ----
  for (i in seq_along(var_names)) {
    var <- var_names[i]
    prefix <- output_names[i]
    label_col <- paste0(prefix, "_label")

    # Only consider rows where input is not NA but label is NA
    unmatched_vals <- unique(df[[var]][is.na(df[[label_col]]) & !is.na(df[[var]])])

    if (length(unmatched_vals) > 0) {
      stop(
        "❌ The following ", length(unmatched_vals), " value(s) in column '", var,
        "' could not be mapped:\n→ ",
        paste(unmatched_vals, collapse = ", "),
        "\nPlease update the classification map to include these values."
      )
    }
  }

  return(df)
}


#USE THIS ONLY FOR ASSESSMENT FILES (SBAC, CAST, ELPAC)
assessment_files_group_labeling <- function(df, var_names, output_names) {
  if (length(var_names) != length(output_names)) {
    stop("You must have the same number of variables as outputs")
  }

  missing_cols <- setdiff(var_names, names(df))
  if (length(missing_cols) > 0) {
    stop(paste("Missing columns in dataframe:", paste(missing_cols, collapse = ", ")))
  }

  # ---- Internal helper: Only prompt if duplicates are also in the data ----
  resolve_duplicates_interactively <- function(classification_map, df, var_names) {
    all_values <- unlist(lapply(classification_map, function(x) x$values))
    dupes <- unique(all_values[duplicated(all_values)])

    # Get all values in the data across the relevant columns
    data_values <- unique(unlist(as.data.frame(df)[, var_names]))

    # Only keep duplicated values that also appear in the data
    relevant_dupes <- intersect(dupes, data_values)
    if (length(relevant_dupes) == 0) return(classification_map)

    message("Duplicate classification values found in your data. Please choose which label to apply:")

    resolved_map <- list()

    for (dup in relevant_dupes) {
      matching_entries <- Filter(function(x) dup %in% x$values, classification_map)

      cat("\nValue:", dup, "\n")
      for (i in seq_along(matching_entries)) {
        cat(i, "-> label:", matching_entries[[i]]$label,
            "| num:", matching_entries[[i]]$num,
            "| group_num:", matching_entries[[i]]$group_num,
            "| group:", matching_entries[[i]]$group, "\n")
      }

      choice <- as.integer(readline(prompt = "Select the number of the preferred option: "))
      selected <- matching_entries[[choice]]
      selected$values <- dup  # Keep only selected value

      # Remove the value from all other entries in the full map
      classification_map <- lapply(classification_map, function(x) {
        x$values <- setdiff(x$values, dup)
        x
      })

      # Add selected entry back
      resolved_map <- append(resolved_map, list(selected))
    }

    # Keep all other non-duplicate or unresolved entries
    unique_map <- Filter(function(x) all(!x$values %in% relevant_dupes), classification_map)
    final_map <- append(unique_map, resolved_map)

    return(final_map)
  }

  # ---- INTERNAL: Hardcoded column-specific value mutation ----
  if ("grade" %in% names(df)) {
    df$grade_original <- df$grade
    df$grade <- dplyr::recode(
      df$grade,
      `99` = 991,
      `8` = 81,
      .default = as.double(df$grade)
    )
  }

  # Sample hardcoded classification map
  classification_map <- list(
    list(values = c(74), label = "Black or African American", num = 1, group_num = 1, group = "Race"),
    list(values = c(75), label = "American Indian or Alaska Native", num = 2, group_num = 1, group = "Race"),
    list(values = c(76), label = "Asian", num = 3, group_num = 1, group = "Race"),
    list(values = c(77), label = "Filipino", num = 4, group_num = 1, group = "Race"),
    list(values = c(78), label = "Hispanic or Latino", num = 5, group_num = 1, group = "Race"),
    list(values = c(79), label = "Native Hawaiian or Pacific Islander", num = 6, group_num = 1, group = "Race"),
    list(values = c(144), label = "Two or More Races", num = 7, group_num = 1, group = "Race"),
    list(values = c(80), label = "White", num = 8, group_num = 1, group = "Race"),
    list(values = c("KN"), label = "Kindergarten", num = 9, group_num = 2, group = "Grade"),
    list(values = c(1, "01"), label = "Grade 1", num = 10, group_num = 2, group = "Grade"),
    list(values = c(2, "02"), label = "Grade 2", num = 11, group_num = 2, group = "Grade"),
    list(values = c(3, "03"), label = "Grade 3", num = 12, group_num = 2, group = "Grade"),
    list(values = c(4, "04"), label = "Grade 4", num = 13, group_num = 2, group = "Grade"),
    list(values = c(5, "05"), label = "Grade 5", num = 14, group_num = 2, group = "Grade"),
    list(values = c(6, "06"), label = "Grade 6", num = 15, group_num = 2, group = "Grade"),
    list(values = c(7, "07"), label = "Grade 7", num = 16, group_num = 2, group = "Grade"),
    list(values = c(81, "08"), label = "Grade 8", num = 17, group_num = 2, group = "Grade"), #manual c() value to avoid forced duplicates
    list(values = c(9, "09"), label = "Grade 9", num = 18, group_num = 2, group = "Grade"),
    list(values = c(10, "10"), label = "Grade 10", num = 19, group_num = 2, group = "Grade"),
    list(values = c(11, "11"), label = "Grade 11", num = 20, group_num = 2, group = "Grade"),
    list(values = c(12, "12"), label = "Grade 12", num = 21, group_num = 2, group = "Grade"),
    list(values = c(13, "13"), label = "All Grades", num = 22, group_num = 2, group = "Grade"),
    list(values = c(14), label = "All High School Grades", num = 23, group_num = 2, group = "Grade"),
    list(values = c(991), label = "Cohort Grade/Graduating Class", num = 24, group_num = 2, group = "Grade"), #manual c() value to avoid forced duplicates
    list(values = c(4), label = "Female", num = 25, group_num = 3, group = "Gender"),
    list(values = c(3), label = "Male", num = 26, group_num = 3, group = "Gender"),
    list(values = c(28), label = "Migrant Youth", num = 27, group_num = 4, group = "Student Subgroup"),
    list(values = c(29), label = "Not Migrant Youth", num = 28, group_num = 4, group = "Student Subgroup"),
    list(values = c(52), label = "Homeless Youth", num = 29, group_num = 4, group = "Student Subgroup"),
    list(values = c(53), label = "Not Homeless Youth", num = 30, group_num = 4, group = "Student Subgroup"),
    list(values = c(240), label = "Foster Youth", num = 31, group_num = 4, group = "Student Subgroup"),
    list(values = c(241), label = "Not Foster Youth", num = 32, group_num = 4, group = "Student Subgroup"),
    list(values = c(50), label = "Armed Forces Family Member", num = 33, group_num = 4, group = "Student Subgroup"),
    list(values = c(51), label = "No Armed Forces Family Member", num = 34, group_num = 4, group = "Student Subgroup"),
    list(values = c(1), label = "All Students", num = 35, group_num = 4, group = "Student Subgroup"),
    list(values = c(128), label = "Student with a Disability", num = 36, group_num = 4, group = "Student Subgroup"),
    list(values = c(99), label = "Student Without a Disability", num = 37, group_num = 4, group = "Student Subgroup"),
    list(values = c(239), label = "Student with a Disability Tested with Alternate Assessment", num = 38, group_num = 5, group = "Test Taken"),
    list(values = c(31), label = "Socioeconomically Disadvantaged", num = 39, group_num = 4, group = "Student Subgroup"),
    list(values = c(111), label = "Not Socioeconomically Disadvantaged", num = 40, group_num = 4, group = "Student Subgroup"),
    list(values = c(6), label = "IFEP, RFEP and EO", num = 41, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(7), label = "IFEP", num = 42, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(8), label = "RFEP", num = 43, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(120), label = "ELs Enrolled Less Than 12 Months", num = 44, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(142), label = "ELs Enrolled 12 Months or More", num = 45, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(160), label = "English Learner", num = 46, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(243), label = "Adult English Learner Only", num = 47, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(180), label = "English Only", num = 48, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(170), label = "Ever-EL", num = 49, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(250), label = "Long-Term English Learner", num = 50, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(251), label = "At-Risk of Becoming LTEL", num = 51, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(252), label = "Never an English Learner", num = 52, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(190), label = "English Language Acquisition Status TBD", num = 53, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(242), label = "English Learner - 1 Year in Program", num = 54, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(243), label = "English Learner - 2 Years in Program", num = 55, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(244), label = "English Learner - 3 Years in Program", num = 56, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(245), label = "English Learner - 4 Years in Program", num = 57, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(246), label = "English Learner - 5 Years in Program", num = 58, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(247), label = "English Learner - 6 Years in Program", num = 59, group_num = 6, group = "English Language Acquisition Status"),
    list(values = c(228), label = "Spanish", num = 60, group_num = 7, group = "First Language"),
    list(values = c(229), label = "Vietnamese", num = 61, group_num = 7, group = "First Language"),
    list(values = c(230), label = "Mandarin (Putonghua)", num = 62, group_num = 7, group = "First Language"),
    list(values = c(231), label = "Arabic", num = 63, group_num = 7, group = "First Language"),
    list(values = c(232), label = "Filipino (Pilipino or Tagalog)", num = 64, group_num = 7, group = "First Language"),
    list(values = c(233), label = "Cantonese", num = 65, group_num = 7, group = "First Language"),
    list(values = c(234), label = "Korean", num = 66, group_num = 7, group = "First Language"),
    list(values = c(235), label = "Hmong", num = 67,group_num = 7,  group = "First Language"),
    list(values = c(236), label = "Punjabi", num = 68, group_num = 7, group = "First Language"),
    list(values = c(237), label = "Russian", num = 69, group_num = 7, group = "First Language"),
    list(values = c(238), label = "All Remaining Languages", num = 70, group_num = 7, group = "First Language"),
    list(values = c(90), label = "Not a High School Graduate", num = 71, group_num = 8, group = "Parent Education"),
    list(values = c(91), label = "High School Graduate", num = 72, group_num = 8, group = "Parent Education"),
    list(values = c(92), label = "Some College (Includes AA Degree)", num = 73, group_num = 8, group = "Parent Education"),
    list(values = c(93), label = "College Graduate", num = 74, group_num = 8, group = "Parent Education"),
    list(values = c(94), label = "Graduate School/Post Graduate", num = 75, group_num = 8, group = "Parent Education"),
    list(values = c(121), label = "Declined to State", num = 76, group_num = 8, group = "Parent Education"),
    list(values = c(201), label = "American Indian or Alaska Native and Economically Disadvantaged", num = 77, group_num = 9, group = "Crosstabs"),
    list(values = c(202), label = "Asian and Economically Disadvantaged", num = 78, group_num = 9, group = "Crosstabs"),
    list(values = c(200), label = "Black or African American and Economically Disadvantaged", num = 79, group_num = 9, group = "Crosstabs"),
    list(values = c(203), label = "Filipino and Economically Disadvantaged", num = 80, group_num = 9, group = "Crosstabs"),
    list(values = c(204), label = "Hispanic or Latino and Economically Disadvantaged", num = 81, group_num = 9, group = "Crosstabs"),
    list(values = c(205), label = "Native Hawaiian or Pacific Islander and Economically Disadvantaged", num = 82, group_num = 9, group = "Crosstabs"),
    list(values = c(206), label = "White and Economically Disadvantaged", num = 83, group_num = 9, group = "Crosstabs"),
    list(values = c(207), label = "Two or More Races and Economically Disadvantaged", num = 84, group_num = 9, group = "Crosstabs"),
    list(values = c(221), label = "American Indian or Alaska Native and Not Economically Disadvantaged", num = 85, group_num = 9, group = "Crosstabs"),
    list(values = c(222), label = "Asian and Not Economically Disadvantaged", num = 86, group_num = 9, group = "Crosstabs"),
    list(values = c(220), label = "Black or African American and Not Economically Disadvantaged", num = 87, group_num = 9, group = "Crosstabs"),
    list(values = c(223), label = "Filipino and Not Economically Disadvantaged", num = 88, group_num = 9, group = "Crosstabs"),
    list(values = c(224), label = "Hispanic or Latino and Not Economically Disadvantaged", num = 89, group_num = 9, group = "Crosstabs"),
    list(values = c(225), label = "Native Hawaiian or Pacific Islander and Not Economically Disadvantaged", num = 90, group_num = 9, group = "Crosstabs"),
    list(values = c(226), label = "White and Not Economically Disadvantaged", num = 91, group_num = 9, group = "Crosstabs"),
    list(values = c(227), label = "Two or More Races and Not Economically Disadvantaged", num = 92, group_num = 9, group = "Crosstabs")
  )

  # ---- Resolve only relevant duplicates ----
  classification_map <- resolve_duplicates_interactively(classification_map, df, var_names)

  # ---- Apply classification across each column ----
  for (i in seq_along(var_names)) {
    var <- var_names[i]
    prefix <- output_names[i]

    label_col <- paste0(prefix, "_label")
    num_col  <- paste0(prefix, "_num")
    group_num_col <- paste0(prefix, "_group_num")
    group_col <- paste0(prefix, "_group")

    df[[label_col]] <- NA_character_
    df[[num_col]]  <- NA_real_
    df[[group_num_col]] <- NA_real_
    df[[group_col]] <- NA_character_

    current_values <- df[[var]]

    for (entry in classification_map) {
      matched <- ifelse(current_values %in% entry$values, TRUE, FALSE)

      if (any(matched, na.rm = TRUE)) {
        df[[label_col]] <- ifelse(is.na(df[[label_col]]) & matched, entry$label, df[[label_col]])
        df[[num_col]]  <- ifelse(is.na(df[[num_col]])  & matched, entry$num,  df[[num_col]])
        df[[group_num_col]] <- ifelse(is.na(df[[group_num_col]]) & matched, entry$group_num, df[[group_num_col]])
        df[[group_col]] <- ifelse(is.na(df[[group_col]]) & matched, entry$group, df[[group_col]])
      }
    }
  }

  if ("grade_original" %in% names(df)) {
    df$grade <- df$grade_original
    df$grade_original <- NULL  # optional: clean up
  }

  # ---- Final validation: Stop if unmapped values are found ----
  for (i in seq_along(var_names)) {
    var <- var_names[i]
    prefix <- output_names[i]
    label_col <- paste0(prefix, "_label")

    # Only consider rows where input is not NA but label is NA
    unmatched_vals <- unique(df[[var]][is.na(df[[label_col]]) & !is.na(df[[var]])])

    if (length(unmatched_vals) > 0) {
      stop(
        "❌ The following ", length(unmatched_vals), " value(s) in column '", var,
        "' could not be mapped:\n→ ",
        paste(unmatched_vals, collapse = ", "),
        "\nPlease update the classification map to include these values."
      )
    }
  }

  return(df)
}

# setwd("T:/CDE data releases/SBAC & CAST Data (2022-present)/2024 data")
# cast24 <- fread(paste0("CA 2024 final CAST file.txt"),
#                     sep = "^",
#                     colClasses = c("County Code" = "character",
#                                    "District Code" = "character",
#                                    "School Code" = "character")) # need these to be character for concat
#
# #Example usage
# cast24 <- assessment_files_group_labeling(cast24,
#                                          var_names = c("Student Group ID", "Grade"),
#                                          output_names = c("student_groups", "grade"))



# Empirically validate that mapping function worked one-to-one ####

validate_group_mapping_tabyl <- function(data, old_col, new_col) {
  require(janitor)
  require(dplyr)

  cross_tab <- data %>%
    tabyl({{ old_col }}, {{ new_col }})

  tab_matrix <- as.matrix(cross_tab[, -1])  # Remove the old_col label column

  # Check how many values > 0 in each row and each column
  row_violations <- which(rowSums(tab_matrix > 0) != 1)
  col_violations <- which(colSums(tab_matrix > 0) != 1)

  if (length(row_violations) == 0 && length(col_violations) == 0) {
    message("\033[32m✅ Each original value maps to exactly one new value.\033[0m")
    return(invisible(TRUE))
  } else {
    message("\033[31m❌ Detected unexpected mappings:\033[0m")
    if (length(row_violations) > 0) {
      message("→ ", length(row_violations), " original value(s) map to multiple new values.")
    }
    if (length(col_violations) > 0) {
      message("→ ", length(col_violations), " new value(s) receive multiple original values.")
    }
    return(invisible(FALSE))
  }
}

# Use this function to figure out if there are CDS codes with multiple org level values ####
get_conflicting_cds <- function(df, cds_col = "cds", org_level_col = "org_level") {
  df %>%
    group_by(across(all_of(cds_col))) %>%
    summarize(unique_org_levels = n_distinct(.data[[org_level_col]]), .groups = "drop") %>%
    filter(unique_org_levels > 1)
}

# Use this function to resolve CDS codes with multiple org level values ###
replace_conflicting_cds <- function(df, conflicting_cds_df, cds_col = "cds", org_level_col = "org_level") {
  conflicting_cds_vec <- conflicting_cds_df %>% pull(!!sym(cds_col))

  df %>%
    mutate(
      altered_cds = if_else(
        .data[[cds_col]] %in% conflicting_cds_vec & .data[[org_level_col]] == "S",
        1L,
        0L
      ),
      !!cds_col := if_else(
        altered_cds == 1L,
        paste0(.data[[cds_col]], "9999"),
        as.character(.data[[cds_col]])
      )
    )
}

# wrapper for the above two functions ####
resolve_conflicting_cds <- function(df, cds_col, org_level_col) {
  # Step 1: Identify conflicting CDS codes
  conflicting_cds_df <- get_conflicting_cds(df, cds_col = cds_col, org_level_col = org_level_col)

  # Step 2: Modify CDS values in the dataset
  df_modified <- replace_conflicting_cds(df, conflicting_cds_df, cds_col = cds_col, org_level_col = org_level_col)

  # Optional: store conflicts as an attribute for later access
  attr(df_modified, "conflicting_cds") <- conflicting_cds_df

  return(df_modified)
}

# Example usage
# chronic24 <- resolve_conflicting_cds(chronic24, cds_col = "cds", org_level_col = "org_level")

# If you want the conflict table
# conflicting_cds <- attr(chronic24, "conflicting_cds")


# Split SBAC file for math and ELA ####

split_sbac_test <- function(data, base_name) {
  if (!"test" %in% names(data)) {
    stop("❌ The dataframe must include a 'test' column.")
  }

  # Split into ELA and Math
  df_ela <- dplyr::filter(data, test == 1)
  df_math <- dplyr::filter(data, test == 2)

  # Check that each split has only one test value
  test_vals_ela <- unique(df_ela$test)
  test_vals_math <- unique(df_math$test)

  if (length(test_vals_ela) != 1 || test_vals_ela != 1) {
    warning("⚠️ Unexpected test value(s) in ELA split: ", paste(test_vals_ela, collapse = ", "))
  }
  if (length(test_vals_math) != 1 || test_vals_math != 2) {
    warning("⚠️ Unexpected test value(s) in Math split: ", paste(test_vals_math, collapse = ", "))
  }

  # Create object names
  name_ela <- paste0(base_name, "_ela")
  name_math <- paste0(base_name, "_math")

  # Assign to global environment
  assign(name_ela, df_ela, envir = .GlobalEnv)
  assign(name_math, df_math, envir = .GlobalEnv)

  # Print confirmation
  cli::cli_h1(paste0("📤 Split Complete for ", base_name))
  cli::cli_alert_success(paste0("✅ Assigned ", name_ela, " — ", nrow(df_ela), " rows, ", ncol(df_ela), " cols."))
  cli::cli_alert_success(paste0("✅ Assigned ", name_math, " — ", nrow(df_math), " rows, ", ncol(df_math), " cols."))

  invisible(list(ela = df_ela, math = df_math))
}


## Helper function for adding "_dim" to FK references ####
fk_dim <- function(name) paste0(name, "_dim")


## validate_combined_key ####

validate_combined_data <- function(
    data,
    key_cols,
    check_pk = TRUE,
    include_year = TRUE,
    id_cols = c("cds","district_code","school_code"),
    show_examples = 10,
    ...
) {
  # data: the *joined* data frame
  # key_cols: columns that should uniquely ID rows (e.g., c("cds","year") or c("district_code","year"))
  # check_pk: run validate_primary_key(data, key_cols, ...) and STOP if it fails
  # include_year: when TRUE and 'year' exists, coverage checks use (id, year); else just id
  # id_cols: which ID columns to audit if present
  # ... : extra args forwarded to validate_primary_key (e.g., sample_n, full_run, seed)

  stopifnot(is.data.frame(data))

  # 0) Column presence for the PK check
  if (!all(key_cols %in% names(data))) {
    missing_cols_str <- paste(setdiff(key_cols, names(data)), collapse = ", ")
    cli::cli_alert_danger("❌ Missing key columns in `data`: {missing_cols_str}.")
  }

  # 1) Create distinct df (across all columns)
  distinct_df <- dplyr::distinct(data)

  # 3) Coverage checks for whichever ID columns exist
  present_ids <- intersect(id_cols, names(data))
  if (!length(present_ids)) {
    id_cols_str <- paste(id_cols, collapse = ", ")
    cli::cli_alert_danger("❌ None of the expected ID columns found: {id_cols_str}.")
  }

  has_year <- include_year && ("year" %in% names(data)) && ("year" %in% names(distinct_df))

  check_one <- function(idc) {

    keys <- if (has_year) c(idc, "year") else idc
    keys_str <- paste(keys, collapse = ", ")

    j_keys <- unique(as.data.frame(data[, keys, drop = FALSE]))
    d_keys <- unique(as.data.frame(distinct_df[, keys, drop = FALSE]))

    # Ensure every key from JOINED appears in DISTINCT
    missing_in_distinct <- dplyr::anti_join(j_keys, d_keys, by = keys)

    if (nrow(missing_in_distinct) == 0) {
      n_ok <- nrow(j_keys)
      cli::cli_alert_success(
        "Successful join for [{keys_str}]: all {n_ok} key(s) from JOINED appear in DISTINCT."
      )

      list(id = idc, keys = keys, pass = TRUE,
           joined_n = nrow(j_keys), distinct_n = nrow(d_keys),
           missing = missing_in_distinct)

    } else {
      n_miss <- nrow(missing_in_distinct)
      cli::cli_alert_danger(
        "❌ {n_miss} key(s) from JOINED are missing in DISTINCT for [{keys_str}]. Examples:"
      )

      print(utils::head(missing_in_distinct, show_examples))

      list(id = idc, keys = keys, pass = FALSE,
           joined_n = nrow(j_keys), distinct_n = nrow(d_keys),
           missing = missing_in_distinct)
    }
  }

  # 3) Optional PK validation on DISTINCT

  if (isTRUE(check_pk)) {

    pk_res <- tryCatch(validate_primary_key(distinct_df, key_cols, ...), error = identity)

    if (inherits(pk_res, "error")) stop(pk_res$message)

    if (is.list(pk_res) && isFALSE(pk_res$pass)) {
      cols_str <- paste(key_cols, collapse = ", ")
      cli::cli_alert_danger(
        "Primary key check failed on DISTINCT data. Fix duplicates/NA in: {cols_str}."
      )
    }

    if (is.list(pk_res) && isTRUE(pk_res$pass)) {
      cols_str <- paste(key_cols, collapse = ", ")
      cli::cli_alert_success(
        "Primary key holds on DISTINCT for [{cols_str}]."
      )
    }
  }

  invisible(lapply(present_ids, check_one))

  distinct_df

}
