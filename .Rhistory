usethis::edit_r_environ()
devtools::document()
devtools::document()
devtools::document()
install.packages("cdetidy")
library(cdetidy)
<<<<<<< HEAD
pkgdown::build_site()
pkgdown::build_site()
.Last.error
pkgdown::build_site()
pkgdown::build_site()
=======
devtools::install_github("jaubele/cdetidy")
usethis::edit_r_environ()
gitcreds::gitcreds_set()
devtools::install_github("jaubele/cdetidy")
library(cdetidy)
devtools::install_github("jaubele/cdetidy")
gitcreds::gitcreds_set()
usethis::create_github_token()
usethis::edit_r_environ()
gitcreds::gitcreds_set()
devtools::install_github("jaubele/cdetidy")
devtools::install_github("jaubele/cdetidy", auth_token = Sys.getenv("GITHUB_PAT"), quiet = FALSE)
gitcreds::gitcreds_set()
usethis::use_git_config(
user.name = "mitchellc2922",
user.email = "cmitchell@ocde.us"
)
devtools::install_github("jaubele/cdetidy")
library(cdetidy)
safe_fwrite_warehouse <- function(data, path = NULL,
char_cols = c("cds", "county_code", "district_code", "school_code"),
compress = FALSE,
n_check = 6,
log_metadata = NULL,
data_year = NULL,
data_source = NULL,
data_description = NA,
user_note = NA,
table_name = NULL,
dim_description = NULL,
log_path = "export_log.csv",
canonical_table_id = NULL,
dimension_type = NULL) {
# Construct default path if not provided
if (is.null(path)) {
required <- c("data_year", "data_source", "table_name")
missing_args <- required[!sapply(list(data_year, data_source, table_name), function(x) !is.null(x))]
if (length(missing_args) > 0) {
stop(paste0("‚ùå To auto-generate the path, you must supply: ",
paste(missing_args, collapse = ", "), "."))
}
}
# Allow shorthand metadata if log_metadata not provided
if (is.null(log_metadata)) {
if (is.null(data_year) || is.null(data_source)) {
stop("‚ùå You must supply either `log_metadata` or both `data_year` and `data_source`.")
}
log_metadata <- list(
data_year = data_year,
data_source = data_source,
data_description = data_description,
user_note = user_note
)
}
# Validate that data_description is provided
if (is.null(log_metadata$data_description) || is.na(log_metadata$data_description) || log_metadata$data_description == "") {
stop("‚ùå Please provide a brief description of what's in this data file using `data_description`.")
}
# Validate required metadata fields
required_fields <- c("data_year", "data_source")
missing_fields <- setdiff(required_fields, names(log_metadata))
if (length(missing_fields) > 0) {
stop("‚ùå Missing required `log_metadata` fields: ",
paste(missing_fields, collapse = ", "))
}
# Validate and standardize data_source values
allowed_sources <- c("Assessment", "CDE", "Dashboard")
input_source <- tolower(log_metadata$data_source)
matched_idx <- match(input_source, tolower(allowed_sources))
if (is.na(matched_idx)) {
stop("‚ùå `data_source` must be one of: ", paste(allowed_sources, collapse = ", "))
}
# Store canonical label for logging (title-case)
log_metadata$data_source <- allowed_sources[matched_idx]
# Store lowercase version separately if you need it for folder names
folder_data_source <- tolower(allowed_sources[matched_idx])
# Validate user_note contains "fact" or "dim"
if (!grepl("\\bfact\\b|\\bdim\\b", log_metadata$user_note, ignore.case = TRUE)) {
stop("‚ùå Your `user_note` must clearly include either 'fact' or 'dim' in the note.")
}
# Determine table suffix
table_type <- tolower(stringr::str_extract(user_note, "\\b(fact|dim)\\b"))
if (is.na(table_type)) {
stop("‚ùå Your `user_note` must clearly include either 'fact' or 'dim'.")
}
# Clean dim_description for file name (if provided)
if (!is.null(dim_description)) {
dim_description <- janitor::make_clean_names(dim_description)
}
# Build enhanced table name
final_table_name <- paste0(
table_name,
if (!is.null(dim_description)) paste0("_", dim_description),
"_", table_type
)
# Auto-generate saving path, T drive specific
if (is.null(path)) {
warehouse_dir <- "T:/Data Warehouse/Warehouse Ready Files"
path <- file.path(warehouse_dir,
as.character(data_year),
"Data Files",
folder_data_source,
paste0(final_table_name, ".csv"))
# Make sure the folder exists
dir.create(dirname(path), recursive = TRUE, showWarnings = FALSE)
}
# Apply compression if requested
if (compress && !grepl("\\.gz$", path)) {
path <- paste0(path, ".gz")
}
# Determine table type from user_note
table_type <- tolower(stringr::str_extract(log_metadata$user_note, "\\b(fact|dim)\\b"))
# Define allowed char columns based on table type
allowed_char_cols <- if (table_type == "dim") {
c("cds", "county_code", "district_code", "school_code")
} else if (table_type == "fact") {
c("cds")
} else {
stop("‚ùå Could not determine table type from `user_note`. Please include 'fact' or 'dim'.")
}
# Filter to only existing columns in the data
existing_char_cols <- intersect(allowed_char_cols, names(data))
# Coerce character columns
data <- data %>%
mutate(across(all_of(existing_char_cols), as.character))
# Write the file
fwrite(data, path)
message("‚úÖ File written: ", path)
# Preview first n rows with character column types enforced
print(fread(path, nrows = n_check,
colClasses = list(character = existing_char_cols)))
# File info
file_info <- file.info(path)
if (is.na(file_info$size)) {
stop("File does not exist or is unreadable: ", path)
}
canonical_table_id <- if (is.null(canonical_table_id)) table_name else canonical_table_id
dimension_type     <- if (is.null(dimension_type)) NA_character_ else dimension_type
valid_dimension_types <- c("universal", "annualized", "other")
if (!is.null(dimension_type) && !is.na(dimension_type)) {
if (!tolower(dimension_type) %in% valid_dimension_types) {
stop("‚ùå `dimension_type` must be one of: ",
paste(valid_dimension_types, collapse = ", "))
}
dimension_type <- tolower(dimension_type)
}
# Construct and write log entry
log_entry <- data.frame(
timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
file_name = paste0(final_table_name, if (compress) ".csv.gz" else ".csv"),
file_path = normalizePath(path),
file_size_MB = round(file_info$size / 1e6, 2),
canonical_table_id = canonical_table_id,
dimension_type = dimension_type,
n_rows = nrow(data),
n_cols = ncol(data),
data_year = log_metadata$data_year,
data_source = log_metadata$data_source,
table_type = table_type,
data_description = log_metadata$data_description,
dim_description = dim_description %||% NA,
user_note = log_metadata$user_note,
user = Sys.info()[["user"]],
stringsAsFactors = FALSE
)
# Read log if it exists
if (file.exists(log_path)) {
existing_log <- read.csv(log_path, stringsAsFactors = FALSE)
# Overwrite row if file_path exists
match_idx <- which(existing_log$canonical_table_id == canonical_table_id)
if (length(match_idx) > 0) {
existing_log[match_idx[1], ] <- log_entry
write.csv(existing_log, file = log_path, row.names = FALSE)
message("üîÅ Existing log entry overwritten for: ", log_entry$file_name)
} else {
write.table(log_entry, file = log_path, append = TRUE, sep = ",",
row.names = FALSE, col.names = FALSE)
message("üìù Log entry appended to: ", log_path)
}
} else {
# New log file
write.table(log_entry, file = log_path, append = FALSE, sep = ",",
row.names = FALSE, col.names = TRUE)
message("üìÑ New log created: ", log_path)
}
invisible(NULL)
}
devtools::document()
>>>>>>> 58eef45878886b4940bdf24abbefa963c3f6ed10
devtools::document()
pkgdown::build_site()
.Last.error
devtools::document()
pkgdown::build_site()
unlink("docs/404.html")
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
devtools::install_github("jaubele/cdetidy")
devtools::document()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
devtools::document()
devtools::document()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
git add .
devtools::document()
pkgdown::build_site()
devtools::document()
pkgdown::build_site()
